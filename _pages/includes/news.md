# ğŸ”¥ News

<ul>
<li><b>2025.09</b>: ğŸ”¥ We introduce <a href="https://huggingface.co/papers/2509.09265"><b>EMPG</b></a>: a new framework that solves the credit assignment bottleneck in long-horizon agent training by fixing a fundamental flaw in policy gradients. ğŸš§ Please find all details in our <a href="https://empgseed-seed.github.io/"><b>project page</b></a>.</li>
<li><b>2025.09</b>: ğŸ‰ We're excited to have contributed to <a href="https://mcpmark.ai/"><b>MCP Mark</b></a>, a solid benchmark for stress-testing comprehensive MCP use. We have open-sourced all details in <a href="https://github.com/eval-sys/mcpmark/">Github</a>. Welcome to join us!</li>
<li><b>2025.08</b>: ğŸ”¥ We introduce <a href="https://arxiv.org/pdf/2508.07999"><b>WideSearch</b></a>: a new benchmark to test if AI agents can handle large-scale, repetitive information gathering â€” the real bottleneck in productivity. ğŸš§ Please find all details in our <a href="https://widesearch-seed.github.io/"><b>project page</b></a>.</li>
<li><b>2025.06</b>: ğŸ‰ Our paper on VLM robustness, "<a href="https://arxiv.org/abs/2504.01308"><b>Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks</b></a>," has been accepted by ICCV 2025! See you in Hawaii!</li>
<li><b>2025.05</b>: ğŸ”¥ Our latest research, <a href="https://arxiv.org/abs/2505.19630"><b>DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue</b></a>, is now available. This work highlights a crucial principle in Human-Agent Interaction: Agents must proactively request necessary information to excel, as humans may not always volunteer it. This "Agent-must-ask" paradigm is central to DoctorAgent-RL's ability to facilitate better task completion in complex multi-turn dialogues. </li>
<li><b>2025.04</b>: ğŸ‰ Thrilled to kick off my new internship with the ByteDance Seed Team!</li>
<li><b>2025.04</b>: ğŸ”¥ Our latest work on VLM robustness, "<a href="https://arxiv.org/abs/2504.01308"><b>Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks</b></a>," has been released on Arxiv. We've open-sourced the <a href="https://huggingface.co/datasets/Jarvis1111/RobustVLGuard"><b>Robust-VLGuard dataset</b></a> and <a href="https://github.com/JarvisUSTC/DiffPure-RobustVLM"><b>DiffPure-VLM defense</b></a>.</li>
<li><b>2025.03</b>: ğŸ‰ Our paper "<a href="https://arxiv.org/abs/2503.15893"><b>UniHDSA: A Unified Relation Prediction Approach for Hierarchical Document Structure Analysis</b></a>" has been accepted by Pattern Recognition Journal!</li>
<li><b>2024.12</b>: ğŸ’» We've launched a new GitHub project: <a href="https://github.com/JarvisUSTC/Awesome-Multimodal-RAG"><b>Awesome-Multimodal-RAG</b></a>! Check out the latest in multimodal RAG and contribute!</li>
<li><b>2024.12</b>: ğŸ¤ We're excited to have contributed to <a href="https://github.com/deepseek-ai/DeepSeek-VL2"><b>DeepSeek-VL2</b></a>, an advanced Vision-Language Model with strong performance and fewer parameters.</li>
</ul>
<details>
<summary> ğŸ”¥ More News</summary>
<ul>
<li><b>2024.08-09</b>: ğŸ—£ï¸ Presented DLAFormer and DRFormer at ICDAR in Athens! Photos can be found <a href="https://photos.app.goo.gl/8aw4mYxDUtA5ELuJ6">here</a>. A memorable experience meeting colleagues and exploring the city.</li>
<li><b>2024.08</b>: âœï¸ The complete version of DLAFormer, titled "<a href="https://arxiv.org/abs/2503.15893"><b>UniHDSA: A Unified Relation Prediction Approach for Hierarchical Document Structure Analysis</b></a>", has been submitted to Pattern Recognition Journal.</li>
<li><b>2024.07</b>: ğŸ‰ Our Detect-Order-Construct have been accepted by Pattern Recognition!</li>
<li><b>2024.06</b>: ğŸ—£ï¸ Our DLAFormer, UniVIE, and DRFormer selected for oral presentation at ICDAR 2024!</li>
<li><b>2024.03</b>: ğŸš€ Azure AI Document Intelligence now supports Hierarchical Document Structure Analysis (HDSA), based on our "<a href="https://arxiv.org/abs/2401.11874"><b>Detect-Order-Construct: A Tree Construction based Approach for Hierarchical Document Structure Analysis</b></a>" paper. Details on <a href="https://arxiv.org/abs/2401.11874">arXiv</a> and the <a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/document-intelligence-preview-adds-more-prebuilts-support-for/ba-p/4084608">official announcement</a>.</li>
<li><b>2024.03</b>: ğŸ’» Source code released for our <a href="https://github.com/JarvisUSTC/Language-Enhanced-CLIP-For-Multi-label-Image-Recognition"><b>Language-Enhanced Image New Category Discovery solution</b></a> from the CVPR 2023 HIT Workshop.</li>
<li><b>2024.02</b>: âœï¸ Our new work on Document Layout Analysis, <b>DLAFormer: A End-to-End Transformer for Document Layout Analysis</b>, submitted to ICDAR 2024.</li>
<li><b>2024.01</b>: ğŸ’¡ Introduced <a href="https://arxiv.org/abs/2401.09220"><b>UniVIE: A Unified Label Space Approach to Visual Information Extraction from Form-like Documents</b></a>! Reframing VIE as relation prediction with a unified label space.</li>
<li><b>2024.01</b>: ğŸ“„ New technical paper released: <a href="https://arxiv.org/abs/2401.09232"><b>Dynamic Relation Transformer for Contextual Text Block Detection</b></a>!</li>
<li><b>2023.12</b>: ğŸ† <a href="https://mp.weixin.qq.com/s/B1r2uJ0bNg_u50vuNI5MPw"><b>2nd Prize</b>, 2023 International Algorithm Case Competition (Visual Prompt Tuning Challenge @ CVPR 2023 HIT Workshop)</a>, <b>200,000 RMB bonus</b>!</li>
<li><b>2023.11</b>: âœï¸ Our new progress on Hierarchical Document Structure Analysis submitted to Pattern Recognition Journal.</li>
<li><b>2023.07</b>: ğŸ‰ "<a href="https://arxiv.org/abs/2303.11615"><b>Robust Table Structure Recognition with Dynamic Queries Enhanced Detection Transformer</b></a>" accepted by Pattern Recognition Journal!</li>
<li><b>2023.04</b>: ğŸ‰ Two papers accepted by <b>ICDAR 2023</b>!</li>
<li><b>2023.03</b>: ğŸ’¡ Proposed a new <a href="https://arxiv.org/abs/2303.11615"><b>Dynamic Queries based Detection Transformer</b></a> for more robust table structure recognition!</li>
<li><b>2022.12</b>: ğŸ† <a href="https://mp.weixin.qq.com/s/Au8oRHbX0Ls2gL4WiBNmqw"><b>2nd Prize</b>, 2022 International Algorithm Case Competition (Panoptic Scene Graph Challenge @ ECCV 2022 SenseHuman Workshop)</a>, <b>100,000 RMB bonus</b>!</li>
<li><b>2022.09</b>: ğŸ‰ One paper accepted by <b>ACM MM 2022</b>!</li>
</ul>
</details>
